{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "baseline.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "OE9wypehaLrZ"
      },
      "source": [
        "### Importing Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "_5y34iNipyr3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 457
        },
        "outputId": "edbb4a59-cde4-4750-c752-4b65964b3e01"
      },
      "source": [
        "from os.path import exists\n",
        "if not exists('enzh_data.zip'):\n",
        "    !wget -O enzh_data.zip https://competitions.codalab.org/my/datasets/download/03e23bd7-8084-4542-997b-6a1ca6dd8a5f\n",
        "    !unzip enzh_data.zip"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-02-24 16:34:13--  https://competitions.codalab.org/my/datasets/download/03e23bd7-8084-4542-997b-6a1ca6dd8a5f\n",
            "Resolving competitions.codalab.org (competitions.codalab.org)... 129.175.22.230\n",
            "Connecting to competitions.codalab.org (competitions.codalab.org)|129.175.22.230|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 FOUND\n",
            "Location: https://newcodalab.lri.fr/prod-private/dataset_data_file/None/630ec/en-zh.zip?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Expires=86400&X-Amz-SignedHeaders=host&X-Amz-Signature=9d2271d56a7b9e0b9797c34ed4bba7547b9750b12942e1d447a398846c4dbaab&X-Amz-Date=20200224T163413Z&X-Amz-Credential=AZIAIOSAODNN7EX123LE%2F20200224%2Fnewcodalab%2Fs3%2Faws4_request [following]\n",
            "--2020-02-24 16:34:13--  https://newcodalab.lri.fr/prod-private/dataset_data_file/None/630ec/en-zh.zip?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Expires=86400&X-Amz-SignedHeaders=host&X-Amz-Signature=9d2271d56a7b9e0b9797c34ed4bba7547b9750b12942e1d447a398846c4dbaab&X-Amz-Date=20200224T163413Z&X-Amz-Credential=AZIAIOSAODNN7EX123LE%2F20200224%2Fnewcodalab%2Fs3%2Faws4_request\n",
            "Resolving newcodalab.lri.fr (newcodalab.lri.fr)... 129.175.15.11\n",
            "Connecting to newcodalab.lri.fr (newcodalab.lri.fr)|129.175.15.11|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 870893 (850K) [application/zip]\n",
            "Saving to: ‘enzh_data.zip’\n",
            "\n",
            "enzh_data.zip       100%[===================>] 850.48K  1.03MB/s    in 0.8s    \n",
            "\n",
            "2020-02-24 16:34:15 (1.03 MB/s) - ‘enzh_data.zip’ saved [870893/870893]\n",
            "\n",
            "Archive:  enzh_data.zip\n",
            "  inflating: dev.enzh.mt             \n",
            "  inflating: dev.enzh.scores         \n",
            "  inflating: dev.enzh.src            \n",
            "  inflating: test.enzh.mt            \n",
            "  inflating: test.enzh.src           \n",
            "  inflating: train.enzh.mt           \n",
            "  inflating: train.enzh.src          \n",
            "  inflating: train.enzh.scores       \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "QsKYMxCSolrx"
      },
      "source": [
        "#### Pre-processing English with GloVe\n",
        "1. Pad the sentence to max_length. Here is 50.\n",
        "2. Do word embedding for each word in a sentence. Word embedding dimension is 100.\n",
        "\n",
        "As a result, each sentence is represented by a (50,100) vector"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "8lc4rdJnrE_Q",
        "outputId": "61d3551b-14c0-4a82-b9a5-7958e88f907a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        }
      },
      "source": [
        "# DON'T RUN IF YOU ALREADY RAN IT IN THE ENGLISH-GERMAN SECTION\n",
        "# Downloading spacy models for english\n",
        "\n",
        "!spacy download en_core_web_md\n",
        "!spacy link en_core_web_md en300"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting en_core_web_md==2.1.0\n",
            "\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_md-2.1.0/en_core_web_md-2.1.0.tar.gz (95.4MB)\n",
            "\u001b[K     |████████████████████████████████| 95.4MB 535kB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: en-core-web-md\n",
            "  Building wheel for en-core-web-md (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for en-core-web-md: filename=en_core_web_md-2.1.0-cp36-none-any.whl size=97126236 sha256=483e2dfbf0e5d3b1e776af5463ffaa18e9e7618586115fd81c0968ae6a987c51\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-4o7p5cwn/wheels/c1/2c/5f/fd7f3ec336bf97b0809c86264d2831c5dfb00fc2e239d1bb01\n",
            "Successfully built en-core-web-md\n",
            "Installing collected packages: en-core-web-md\n",
            "Successfully installed en-core-web-md-2.1.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_md')\n",
            "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
            "/usr/local/lib/python3.6/dist-packages/en_core_web_md -->\n",
            "/usr/local/lib/python3.6/dist-packages/spacy/data/en300\n",
            "You can now load the model via spacy.load('en300')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "fx3Ja9zWFDj2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "255e8962-8459-4547-8b10-fd0b5eef080f"
      },
      "source": [
        "import torchtext\n",
        "import spacy\n",
        "\n",
        "#Embeddings\n",
        "glove = torchtext.vocab.GloVe(name='6B', dim=100)\n",
        "\n",
        "#tokenizer model\n",
        "nlp_en =spacy.load('en300')\n",
        "\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ".vector_cache/glove.6B.zip: 862MB [06:27, 2.23MB/s]                           \n",
            "100%|█████████▉| 398284/400000 [00:40<00:00, 18116.24it/s]"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "2BUi2QiCIi9y",
        "outputId": "b741b879-33f0-4b8b-a80d-a64798f094d0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "#ENGLISH EMBEDDINGS methods from the section GERMAN-ENGLISH\n",
        "# The difference from previous section is that we will use Glove embeddings directly because we are using a smaller model that spacy doesn't have\n",
        "# We add a method to compute the word embedding and a method to compute the sentence embedding by averaging the word vectors\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from nltk import download\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "#downloading stopwords from the nltk package\n",
        "download('stopwords') #stopwords dictionary, run once\n",
        "stop_words_en = set(stopwords.words('english'))\n",
        "max_length = 50\n",
        "\n",
        "def preprocess(sentence,nlp):\n",
        "    text = sentence.lower()\n",
        "    doc = [token.lemma_ for token in  nlp.tokenizer(text)]\n",
        "    # sentence = sentence.replace('\\r','')\n",
        "    # sentence = sentence.replace('\\n','')\n",
        "    # doc = sentence.split(' ')\n",
        "    #print(doc)\n",
        "    #doc = [word for word in doc if word not in stop_words_en]\n",
        "    doc = [word for word in doc if word.isalpha()] #restricts string to alphabetic characters only\n",
        "    \n",
        "    return doc\n",
        "\n",
        "def get_word_vector(embeddings, word):\n",
        "    try:\n",
        "      vec = embeddings.vectors[embeddings.stoi[word]]\n",
        "      if vec is not None:\n",
        "        return vec\n",
        "    except KeyError:\n",
        "      #print(f\"Word {word} does not exist\")\n",
        "      return torch.zeros((100))\n",
        "\n",
        "def get_sentence_vector(embeddings,line):\n",
        "  vectors = []\n",
        "  for w in line:\n",
        "    emb = get_word_vector(embeddings,w)\n",
        "    #do not add if the word is out of vocabulary\n",
        "    if emb is not None:\n",
        "      vectors.append(emb)\n",
        "    \n",
        "   \n",
        "  return np.stack(vectors)\n",
        "\n",
        "\n",
        "def get_embeddings(f,embeddings,lang):\n",
        "  file = open(f) \n",
        "  lines = file.readlines() \n",
        "  sentences_vectors =[]\n",
        "  for l in lines:\n",
        "    sentence= preprocess(l,lang)\n",
        "    while len(sentence)<max_length:\n",
        "      sentence.append('<pad>')\n",
        "    try:\n",
        "      vec = get_sentence_vector(embeddings,sentence)\n",
        "      sentences_vectors.append(vec)\n",
        "    except:\n",
        "      sentences_vectors.append(0)\n",
        "\n",
        "  return sentences_vectors\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-jW3S2-rs6BV",
        "outputId": "f5eca8e8-8d2c-47d3-c69f-fb449735c521",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        }
      },
      "source": [
        "\n",
        "!wget -c https://github.com/Tony607/Chinese_sentiment_analysis/blob/master/data/chinese_stop_words.txt\n",
        "\n",
        "!wget -O zh.zip http://vectors.nlpl.eu/repository/20/35.zip\n",
        "\n",
        "!unzip zh.zip \n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-02-24 16:48:23--  https://github.com/Tony607/Chinese_sentiment_analysis/blob/master/data/chinese_stop_words.txt\n",
            "Resolving github.com (github.com)... 192.30.255.113\n",
            "Connecting to github.com (github.com)|192.30.255.113|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: ‘chinese_stop_words.txt’\n",
            "\n",
            "\rchinese_stop_words.     [<=>                 ]       0  --.-KB/s               \rchinese_stop_words.     [ <=>                ] 419.26K  --.-KB/s    in 0.03s   \n",
            "\n",
            "2020-02-24 16:48:23 (13.4 MB/s) - ‘chinese_stop_words.txt’ saved [429325]\n",
            "\n",
            "--2020-02-24 16:48:24--  http://vectors.nlpl.eu/repository/20/35.zip\n",
            "Resolving vectors.nlpl.eu (vectors.nlpl.eu)... 129.240.189.225\n",
            "Connecting to vectors.nlpl.eu (vectors.nlpl.eu)|129.240.189.225|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1458485917 (1.4G) [application/zip]\n",
            "Saving to: ‘zh.zip’\n",
            "\n",
            "zh.zip              100%[===================>]   1.36G  17.6MB/s    in 96s     \n",
            "\n",
            "2020-02-24 16:50:01 (14.5 MB/s) - ‘zh.zip’ saved [1458485917/1458485917]\n",
            "\n",
            "Archive:  zh.zip\n",
            "  inflating: LIST                    \n",
            "  inflating: meta.json               \n",
            "  inflating: model.bin               \n",
            "  inflating: model.txt               \n",
            "  inflating: README                  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "uDUbXQ4aMv1K",
        "outputId": "d6f33283-d548-4046-814a-c42619d547ce",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "from gensim.models import Word2Vec\n",
        "from gensim.models import KeyedVectors\n",
        "\n",
        "wv_from_bin = KeyedVectors.load_word2vec_format(\"model.bin\", binary=True) "
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:402: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "uhZ3HtrdodcW"
      },
      "source": [
        "#### Pre-processing Chinese"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "her8c6oJFWAa"
      },
      "source": [
        "Do the same as English."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-LA9N1zgsSQl",
        "colab": {}
      },
      "source": [
        "\n",
        "import string\n",
        "import jieba\n",
        "import gensim \n",
        "import spacy\n",
        "import numpy as np\n",
        "\n",
        "stop_words = [ line.rstrip() for line in open('./chinese_stop_words.txt',\"r\", encoding=\"utf-8\") ]\n",
        "\n",
        "\n",
        "def get_sentence_vector_zh(line):\n",
        "  vectors = []\n",
        "  for w in line:\n",
        "    try:\n",
        "      emb = wv_from_bin[w]\n",
        "      vectors.append(emb)\n",
        "    except:\n",
        "      emb = np.zeros(100)\n",
        "      vectors.append(emb)\n",
        "  if vectors:\n",
        "    return np.stack(vectors)\n",
        "  else:\n",
        "    return np.zeros((100,))\n",
        "\n",
        "\n",
        "def processing_zh(sentence):\n",
        "  seg_list = jieba.lcut(sentence,cut_all=False)\n",
        "  doc = [word for word in seg_list if word not in stop_words]\n",
        "  docs = [e for e in doc if e.isalnum()]\n",
        "  if(len(docs)>max_length):\n",
        "    print(docs)\n",
        "  while(len(docs)<max_length):\n",
        "    docs.append('<pad>')\n",
        "  return docs\n",
        "\n",
        "\n",
        "def get_sentence_embeddings_zh(f):\n",
        "  file = open(f) \n",
        "  lines = file.readlines() \n",
        "  sentences_vectors =[]\n",
        "  for l in lines:\n",
        "    sent  = processing_zh(l)\n",
        "    vec = get_sentence_vector_zh(sent)\n",
        "\n",
        "    if vec is not None:\n",
        "      sentences_vectors.append(vec)\n",
        "    else:\n",
        "      print(l)\n",
        "  return sentences_vectors\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nzFBYFYuHWdx",
        "colab_type": "text"
      },
      "source": [
        "Import data and get sentence embedding."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "6zVjor64tR8D",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        },
        "outputId": "4ba7a9ef-6b7f-467f-bd2f-44c31c95ea56"
      },
      "source": [
        "import spacy\n",
        "import torchtext\n",
        "from torchtext import data\n",
        "\n",
        "\n",
        "zh_train_mt = get_sentence_embeddings_zh(\"./train.enzh.mt\")\n",
        "zh_train_src = get_embeddings(\"./train.enzh.src\",glove,nlp_en)\n",
        "f_train_scores = open(\"./train.enzh.scores\",'r')\n",
        "zh_train_scores = f_train_scores.readlines()\n",
        "\n",
        "\n",
        "zh_val_src = get_embeddings(\"./dev.enzh.src\",glove,nlp_en)\n",
        "zh_val_mt = get_sentence_embeddings_zh(\"./dev.enzh.mt\")\n",
        "f_val_scores = open(\"./dev.enzh.scores\",'r')\n",
        "zh_val_scores = f_val_scores.readlines()\n",
        "\n",
        "\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Building prefix dict from the default dictionary ...\n",
            "Dumping model to file cache /tmp/jieba.cache\n",
            "Loading model cost 0.873 seconds.\n",
            "Prefix dict has been built successfully.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "N-pXbaJzExaE",
        "outputId": "5dadb2f1-8f0d-4b52-c3ab-e014e01a9cfb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        }
      },
      "source": [
        "print(f\"Training mt: {len(zh_train_mt)} Training src: {len(zh_train_src)}\")\n",
        "print()\n",
        "print(f\"Validation mt: {len(zh_val_mt)} Validation src: {len(zh_val_src)}\")\n",
        "print((zh_train_mt[0].shape))\n",
        "print((zh_train_src[0].shape))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training mt: 7000 Training src: 7000\n",
            "\n",
            "Validation mt: 1000 Validation src: 1000\n",
            "(50, 100)\n",
            "(50, 100)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y910T2QoGUI6",
        "colab_type": "code",
        "outputId": "afb113a5-8f29-4ae3-fd5b-3d8547e9f66c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "print(np.array(zh_train_src).shape)\n",
        "print(np.array(zh_train_mt).shape)\n",
        "print(np.array(zh_val_mt).shape)\n",
        "print(np.array(zh_val_src).shape)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(7000, 50, 100)\n",
            "(7000, 50, 100)\n",
            "(1000, 50, 100)\n",
            "(1000, 50, 100)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QiEabAg-IqYr",
        "colab_type": "text"
      },
      "source": [
        "Combine two languages in a new dimension. Input shape is (7000, 50, 100, 2), where 50 is sentence max length and 100 is word embedding dimension."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ljBHJpa9ATNf",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from sklearn import preprocessing\n",
        "\n",
        "\n",
        "X_train= [np.array(zh_train_src),np.array(zh_train_mt)]\n",
        "X_train_zh = np.array(X_train)\n",
        "X_train_zh = X_train_zh.transpose(1,2,3,0)\n",
        "\n",
        "\n",
        "\n",
        "X_val = [np.array(zh_val_src),np.array(zh_val_mt)]\n",
        "X_val_zh = np.array(X_val)\n",
        "X_val_zh = X_val_zh.transpose(1,2,3,0)\n",
        "\n",
        "#Scores\n",
        "train_scores = np.array(zh_train_scores).astype(float)\n",
        "y_train_zh =train_scores\n",
        "# pre = preprocessing.MinMaxScaler()\n",
        "# y_train_zh = pre.fit_transform(y_train_zh)\n",
        "# y_train_zh = y_train_zh.reshape(7000,)\n",
        "\n",
        "val_scores = np.array(zh_val_scores).astype(float)\n",
        "y_val_zh =val_scores"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BFp8pXFvMdey",
        "colab_type": "code",
        "outputId": "686f643c-8b82-41f8-a628-b64ec52d6ed9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "print(X_train_zh.shape)\n",
        "print(X_val_zh.shape)\n",
        "print(y_train_zh.shape)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(7000, 50, 100, 2)\n",
            "(1000, 50, 100, 2)\n",
            "(7000,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ym8BNHrCI8WJ",
        "colab_type": "text"
      },
      "source": [
        "Use Keras to build a Convolutional Neural Network."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F-ce5NjxCHd_",
        "colab_type": "code",
        "outputId": "7813ddbb-4069-4dbe-e8d0-0476266b3593",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 79
        }
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense,Conv2D,Dropout, Activation, Flatten,MaxPooling2D,MaxPooling1D\n",
        "from scipy.stats.stats import pearsonr\n",
        "def baseline_model():\n",
        "    # create model\n",
        "    model = Sequential()\n",
        "    model.add(Conv2D(16,(4,4),activation='relu',input_shape=(50,100,2)))\n",
        "    model.add(MaxPooling2D(2,2))\n",
        "    model.add(Conv2D(8,(4,4),activation='relu'))\n",
        "    model.add(MaxPooling2D(2,2))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(32,init='normal', activation='relu'))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(8,init='normal',activation='sigmoid'))\n",
        "    model.add(Dense(1, init='normal'))\n",
        "      # Compile model\n",
        "    model.compile(loss='mean_squared_error', optimizer='adam',metrics=['accuracy']) \n",
        "    return model"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8FwHms0cJK8e",
        "colab_type": "text"
      },
      "source": [
        "Train and test the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UWLnmiG8JCBD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 558
        },
        "outputId": "2626c7ff-4458-4e62-a553-8c29b2ad4c2e"
      },
      "source": [
        "model = baseline_model()\n",
        "model.fit(X_train_zh, y_train_zh, nb_epoch=10, batch_size=64,validation_data=(X_val_zh,y_val_zh))\n",
        "predictions = model.predict(X_val_zh)\n",
        "predictions = predictions.astype(np.float64).reshape(1000,)\n",
        "print(predictions.shape)\n",
        "pearson = pearsonr(y_val_zh, predictions)\n",
        "print(f'Pearson {pearson[0]}')"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:12: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(32, activation=\"relu\", kernel_initializer=\"normal\")`\n",
            "  if sys.path[0] == '':\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:14: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(8, activation=\"sigmoid\", kernel_initializer=\"normal\")`\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:15: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(1, kernel_initializer=\"normal\")`\n",
            "  from ipykernel import kernelapp as app\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:2: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
            "  \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 7000 samples, validate on 1000 samples\n",
            "Epoch 1/10\n",
            "7000/7000 [==============================] - 2s 241us/step - loss: 0.8586 - acc: 0.0000e+00 - val_loss: 0.8239 - val_acc: 0.0000e+00\n",
            "Epoch 2/10\n",
            "7000/7000 [==============================] - 1s 183us/step - loss: 0.8567 - acc: 0.0000e+00 - val_loss: 0.8195 - val_acc: 0.0000e+00\n",
            "Epoch 3/10\n",
            "7000/7000 [==============================] - 1s 182us/step - loss: 0.8546 - acc: 0.0000e+00 - val_loss: 0.8184 - val_acc: 0.0000e+00\n",
            "Epoch 4/10\n",
            "7000/7000 [==============================] - 1s 183us/step - loss: 0.8546 - acc: 0.0000e+00 - val_loss: 0.8207 - val_acc: 0.0000e+00\n",
            "Epoch 5/10\n",
            "7000/7000 [==============================] - 1s 183us/step - loss: 0.8535 - acc: 0.0000e+00 - val_loss: 0.8145 - val_acc: 0.0000e+00\n",
            "Epoch 6/10\n",
            "7000/7000 [==============================] - 1s 188us/step - loss: 0.8464 - acc: 0.0000e+00 - val_loss: 0.8042 - val_acc: 0.0000e+00\n",
            "Epoch 7/10\n",
            "7000/7000 [==============================] - 1s 184us/step - loss: 0.8281 - acc: 0.0000e+00 - val_loss: 0.7866 - val_acc: 0.0000e+00\n",
            "Epoch 8/10\n",
            "7000/7000 [==============================] - 1s 184us/step - loss: 0.8165 - acc: 0.0000e+00 - val_loss: 0.7855 - val_acc: 0.0000e+00\n",
            "Epoch 9/10\n",
            "7000/7000 [==============================] - 1s 185us/step - loss: 0.8047 - acc: 0.0000e+00 - val_loss: 0.7818 - val_acc: 0.0000e+00\n",
            "Epoch 10/10\n",
            "7000/7000 [==============================] - 1s 183us/step - loss: 0.7918 - acc: 0.0000e+00 - val_loss: 0.7773 - val_acc: 0.0000e+00\n",
            "(1000,)\n",
            "Pearson 0.246305443793236\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "cuIsX8LNiOJm"
      },
      "source": [
        "### Writing Results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "5SQlcfiCITuC",
        "colab": {}
      },
      "source": [
        "import os\n",
        "\n",
        "def writeScores(method_name,scores):\n",
        "    fn = \"predictions.txt\"\n",
        "    print(\"\")\n",
        "    with open(fn, 'w') as output_file:\n",
        "        for idx,x in enumerate(scores):\n",
        "            #out =  metrics[idx]+\":\"+str(\"{0:.2f}\".format(x))+\"\\n\"\n",
        "            #print(out)\n",
        "            output_file.write(f\"{x}\\n\")\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aiqn4KIyzHd9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#EN_ZH\n",
        "\n",
        "zh_test_mt = get_sentence_embeddings_zh(\"./test.enzh.mt\")\n",
        "zh_test_src = get_embeddings(\"./test.enzh.src\",glove,nlp_en)\n",
        "\n",
        "X_test = [np.array(zh_test_src),np.array(zh_test_mt)]\n",
        "X_test_zh = np.array(X_test)\n",
        "X_test_zh = X_test_zh.transpose(1,2,3,0)\n",
        "\n",
        "#Predict\n",
        "predictions_zh = model.predict(X_test_zh)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "r-nDAsi3Xt-4",
        "outputId": "29766f25-e21f-4bba-c4df-64e020f1040e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#EN_ZH\n",
        "\n",
        "from google.colab import files\n",
        "from zipfile import ZipFile\n",
        "\n",
        "\n",
        "writeScores(\"CNN\",predictions_zh)\n",
        "\n",
        "with ZipFile(\"en-zh_cnn.zip\",\"w\") as newzip:\n",
        "\tnewzip.write(\"predictions.txt\")\n",
        " \n",
        "files.download('en-zh_cnn.zip') \n"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}